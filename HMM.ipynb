{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy\n",
    "from collections import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    \"\"\"\n",
    "    Simple class to represent a Hidden Markov Model.\n",
    "    \"\"\"\n",
    "    def __init__(self, order, initial_distribution, emission_matrix, transition_matrix):\n",
    "        self.order = order\n",
    "        self.initial_distribution = initial_distribution\n",
    "        self.emission_matrix = emission_matrix\n",
    "        self.transition_matrix = transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pos_file(filename):\n",
    "    \"\"\"\n",
    "    Parses an input tagged text file.\n",
    "    Input:\n",
    "    filename --- the file to parse\n",
    "    Returns: \n",
    "    The file represented as a list of tuples, where each tuple \n",
    "    is of the form (word, POS-tag).\n",
    "    A list of unique words found in the file.\n",
    "    A list of unique POS tags found in the file.\n",
    "    \"\"\"\n",
    "    file_representation = []\n",
    "    unique_words = set()\n",
    "    unique_tags = set()\n",
    "    f = open(str(filename), \"r\")\n",
    "    for line in f:\n",
    "        if len(line) < 2 or len(line.split(\"/\")) != 2:\n",
    "            continue\n",
    "        \n",
    "        word = line.split(\"/\")[0].replace(\" \", \"\").replace(\"\\t\", \"\").strip()\n",
    "        tag = line.split(\"/\")[1].replace(\" \", \"\").replace(\"\\t\", \"\").strip()\n",
    "        file_representation.append( (word, tag) )\n",
    "        unique_words.add(word)\n",
    "        unique_tags.add(tag)\n",
    "    \n",
    "    f.close()\n",
    "    \n",
    "    return file_representation, unique_words, unique_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pos_file_modified(training_data_file):\n",
    "    \"\"\"\n",
    "    A modified verysion of read_pos that only returns the file representation\n",
    "    Input: training data file, a text file\n",
    "    Output: The file represented as a list of tuples, where each tuple \n",
    "    is of the form (word, POS-tag).\n",
    "    \"\"\"\n",
    "    file_representation = []\n",
    "\n",
    "    #open file\n",
    "    f = open(str(training_data_file), \"r\")\n",
    "    for line in f:\n",
    "        if len(line) < 2 or len(line.split(\"/\")) != 2:\n",
    "            continue\n",
    "\n",
    "        #split the string up\n",
    "        word = line.split(\"/\")[0].replace(\" \", \"\").replace(\"\\t\", \"\").strip()\n",
    "        tag = line.split(\"/\")[1].replace(\" \", \"\").replace(\"\\t\", \"\").strip()\n",
    "        file_representation.append( (word, tag) )\n",
    "\n",
    "    # close the file\n",
    "    f.close()\n",
    "    return file_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('New', 'NNP'), ('Deal', 'NNP'), ('was', 'VBD'), ('a', 'DT'), ('series', 'NN'), ('of', 'IN'), ('domestic', 'JJ'), ('programs', 'NNS'), ('enacted', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('between', 'IN'), ('1933', 'CD'), ('and', 'CC'), ('1936', 'CD'), (',', ','), ('and', 'CC'), ('a', 'DT'), ('few', 'JJ'), ('that', 'WDT'), ('came', 'VBD'), ('later', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "################################### Testing RPOSFM \n",
    "print (read_pos_file_modified(\"onesentence.txt\"))\n",
    "#expects The file represented as a list of tuples, where each tuple is of the form (word, POS-tag).\n",
    "#passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_file(test_file):\n",
    "    \"\"\"\n",
    "    Parses a test file into a list of lists, where the inner lists are sentences\n",
    "    Input: A testing data file, test_file\n",
    "    Outputs: a list of words in the file, and a list of lists as described above\n",
    "    \"\"\"\n",
    "\n",
    "    # open the file\n",
    "    f = open(test_file, \"r\")\n",
    "\n",
    "    #split the file into a list\n",
    "    list_of_words = f.read().split()\n",
    "    testing_block = []\n",
    "    L = len(list_of_words)\n",
    "    count = 0\n",
    "\n",
    "    while count < L:\n",
    "\n",
    "        #container for each sentence\n",
    "        sentence = []\n",
    "        for word in list_of_words:\n",
    "\n",
    "            #add word to the sentence\n",
    "            sentence.append(word)\n",
    "            count += 1\n",
    "            if word == \".\":\n",
    "\n",
    "                # add sentence to the testing block\n",
    "                testing_block.append(sentence)\n",
    "                sentence = []\n",
    "\n",
    "    return list_of_words, testing_block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## TESTING PARSE  ####################################\n",
    "# print (parse_test_file(\"testdata_untagged.txt\"))\n",
    "#expect the test data parsed into a list of list\n",
    "#passes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_data(training_data_file, percent):\n",
    "    \"\"\"\n",
    "    Inputs: training_data_file, a text file of tagged training data \n",
    "            percent: a decimal represeting the amount of data you want to build a model on \n",
    "    Output: partitioned data, a sequence of (word, tag) tuples\n",
    "    \"\"\"\n",
    "    #read in data\n",
    "    data = read_pos_file_modified(training_data_file)\n",
    "\n",
    "    #get the total length of the training data\n",
    "    tokens = len(data)\n",
    "\n",
    "    #get the amount of tokens you want to keep \n",
    "    end_token = int(tokens*percent)\n",
    "\n",
    "    partitioned_data = []\n",
    "\n",
    "    #iterate until you read the end number of tokens\n",
    "    for i in range(end_token):\n",
    "        partitioned_data.append(data[i])\n",
    "\n",
    "    return partitioned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################# TESTING Wrangle Data #############################################\n",
    "#print (wrangle_data(\"training.txt\", 0.01))[1]\n",
    "# expects 1 percent of the training data\n",
    "# passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words_and_tags(data):\n",
    "    \"\"\"\n",
    "    Gets the unique words and tags in a data set\n",
    "    Input: Data in the representation returned by read pos\n",
    "    Output: 2 sets, one that holds the unique words and one that holds the unique tags\n",
    "    \"\"\"\n",
    "\n",
    "    #init empty sets\n",
    "    unique_words = set([])\n",
    "    unique_tags = set([])\n",
    "\n",
    "\n",
    "    for pair in data:\n",
    "\n",
    "        #add the word if its not already been seen\n",
    "        if pair[0] not in unique_words:\n",
    "            unique_words.add(pair[0])\n",
    "\n",
    "        #add the tag if its not already been seen\n",
    "        if pair[1] not in unique_tags:\n",
    "            unique_tags.add(pair[1])\n",
    "\n",
    "    return unique_words, unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### TESTING GET UNIQUE WORDS/TAGS ##########################\n",
    "#fifty = wrangle_data(\"training.txt\", 0.5)\n",
    "#print (get_unique_words_and_tags(fifty))\n",
    "# expects two sets with unique words and tags, on fifty percent of the training data\n",
    "# passes\n",
    "\n",
    "fifty = wrangle_data(\"training.txt\", 0.1)\n",
    "#print (get_unique_words_and_tags(fifty))\n",
    "# expects two sets with unique words and tags, on ten percent of the training data\n",
    "# passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts(training_data, order):\n",
    "    \"\"\"\n",
    "    Function that computes different relevant counts about the input file\n",
    "\n",
    "    Input: Training data, a list of (word, POS-tag) pairs returned by the function read_pos_file, \n",
    "           Order, the order of the hidden markov model\n",
    "\n",
    "    Output : If the HMM order is 2, the function returns a tuple consisting of:\n",
    "                the number of tokens in training data\n",
    "                dictionary that contains that contains C(ti,wi) for every unique tag and unique word (keys correspond to tags)\n",
    "                    The number of times word wi is tagged with ti\n",
    "                a dictionary that contains  C(ti)\n",
    "                    The number of times tag ti appears\n",
    "                a dictionary that contains C(ti-1, ti)\n",
    "                    The number of times the tag sequence ti-1, ti appears\n",
    "             If the HMM order is 3, the function returns a tuple consisting of:\n",
    "                the number of tokens in training data\n",
    "                dictionary that contains that contains C(ti,wi) for every unique tag and unique word (keys correspond to tags)\n",
    "                a dictionary that contains  C(ti)\n",
    "                a dictionary that contains  C(ti-1, ti)\n",
    "                a dictionary that contains  C(ti-2, ti-1, ti)\n",
    "                    The number of times the tag sequence ti-2, ti-1, ti appears\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # get the number of tokens in the training set \n",
    "    numtokens = len(training_data)\n",
    "\n",
    "    # counts the number of times word wi is taked with tag ti\n",
    "    word2tag_dict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # counts the number of times tag ti appears\n",
    "    tag_count_dict = defaultdict(int)\n",
    "\n",
    "    # counts the number of times the tag sequence ti-1, ti appears\n",
    "    bigramdict = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # counts the number of times the tag sequence ti-2, ti-1, ti appears\n",
    "    trigramdict = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "    for i in range(0, numtokens):\n",
    "\n",
    "        #increment the tag_count_dict\n",
    "        tag_count_dict[training_data[i][1]] += 1\n",
    "\n",
    "        #increment the word2tag_dict\n",
    "        word2tag_dict[training_data[i][1]][training_data[i][0]]+=1\n",
    "\n",
    "        if i > 0:\n",
    "\n",
    "            if training_data[i-1][1] != \".\":\n",
    "\n",
    "                #increment the bigram dict\n",
    "                bigramdict[training_data[i-1][1]][training_data[i][1]] += 1\n",
    "\n",
    "        if order > 2:\n",
    "\n",
    "            if i > 1:\n",
    "\n",
    "                if (training_data[i-2][1] != \".\") and (training_data[i-1][1] != \".\"):\n",
    "\n",
    "                    #increment the trigram dict\n",
    "                    trigramdict[training_data[i-2][1]][training_data[i-1][1]][training_data[i][1]] += 1\n",
    "\n",
    "\n",
    "\n",
    "    if order == 2:\n",
    "        return(numtokens, word2tag_dict, tag_count_dict, bigramdict)\n",
    "    else:\n",
    "        return(numtokens, word2tag_dict, tag_count_dict, bigramdict, trigramdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "{'DT': defaultdict(<class 'int'>, {'The': 1, 'a': 2, 'the': 1}), 'NNP': defaultdict(<class 'int'>, {'New': 1, 'Deal': 1, 'United': 1}), 'VBD': defaultdict(<class 'int'>, {'was': 1, 'came': 1}), 'NN': defaultdict(<class 'int'>, {'series': 1}), 'IN': defaultdict(<class 'int'>, {'of': 1, 'in': 1, 'between': 1}), 'JJ': defaultdict(<class 'int'>, {'domestic': 1, 'few': 1}), 'NNS': defaultdict(<class 'int'>, {'programs': 1}), 'VBN': defaultdict(<class 'int'>, {'enacted': 1}), 'NNPS': defaultdict(<class 'int'>, {'States': 1}), 'CD': defaultdict(<class 'int'>, {'1933': 1, '1936': 1}), 'CC': defaultdict(<class 'int'>, {'and': 2}), ',': defaultdict(<class 'int'>, {',': 1}), 'WDT': defaultdict(<class 'int'>, {'that': 1}), 'RB': defaultdict(<class 'int'>, {'later': 1}), '.': defaultdict(<class 'int'>, {'.': 1})}\n",
      "{'DT': 4, 'NNP': 3, 'VBD': 2, 'NN': 1, 'IN': 3, 'JJ': 2, 'NNS': 1, 'VBN': 1, 'NNPS': 1, 'CD': 2, 'CC': 2, ',': 1, 'WDT': 1, 'RB': 1, '.': 1}\n"
     ]
    }
   ],
   "source": [
    "####### TESTING COMPUTE COUNTS ##################\n",
    "tinydata = read_pos_file('onesentence.txt')\n",
    "#print(tinydata[0])\n",
    "#Order 2 test on a single sentence\n",
    "print(compute_counts(tinydata[0], 2)[0])\n",
    "print(dict(compute_counts(tinydata[0], 2)[1]))\n",
    "print(dict(compute_counts(tinydata[0], 2)[2]))\n",
    "\n",
    "\n",
    "\n",
    "#expect a dictionary with the correct amount of words in the sentence\n",
    "#passes\n",
    "\n",
    "# Order 3 test on a single sentence\n",
    "#print(dict(compute_counts(tinydata[0], 3)))\n",
    "# expect a dictionary with the correct amount of words in the sentence\n",
    "# passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_initial_distribution(training_data, order):\n",
    "    \"\"\"\n",
    "    Function that computes the intitial distributions of words, pi_1 and pi_2\n",
    "\n",
    "    Input: Training data, a list of (word, POS-tag) pairs returned by the function read_pos_file, \n",
    "           Order, the order of the hidden markov model\n",
    "\n",
    "    Output: If order = 2:\n",
    "                Returns a one dim dictionary pi_1, that maps a tag to its emission probability\n",
    "            If order = 3:\n",
    "                Returns a 2 dim dictionary pi_2 , that maps a bigram to its emmission probabiliity\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    numtokens = len(training_data)\n",
    "\n",
    "    # initialize the pi dictionaries\n",
    "    pi_1 = defaultdict(int)\n",
    "    pi_2 = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    if order==2:\n",
    "\n",
    "        # the first tag is the second element of the first tuple\n",
    "        first_tag = training_data[0][1]\n",
    "\n",
    "        # increment the total count by 1\n",
    "        pi_1[first_tag] += 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        # aceess the first and second tags\n",
    "        first_tag = training_data[0][1]\n",
    "        second_tag = training_data[1][1]\n",
    "\n",
    "        # increment the dictionary\n",
    "        pi_2[first_tag][second_tag] += 1\n",
    "\n",
    "    # set the total counts to be 1, we will normalize later\n",
    "    order2count = 1\n",
    "    order3count = 1\n",
    "\n",
    "    for i in range(order -1, numtokens - order + 1):\n",
    "\n",
    "        if order == 2:\n",
    "\n",
    "            #if we encounter a period, we know we have the beginning of a sentence\n",
    "            if training_data[i-1][1] == \".\":\n",
    "\n",
    "                # increment the count by 1\n",
    "                pi_1[training_data[i][1]] += 1\n",
    "                order2count += 1\n",
    "\n",
    "        if order == 3:\n",
    "\n",
    "            # if two words ago was a period, then we have a bigram that is at the begining of a word\n",
    "            # will fail for 1 word sentences\n",
    "            if training_data[i-2][1] == \".\":\n",
    "                pi_2[training_data[i-1][1]][training_data[i][1]] += 1 \n",
    "                order3count +=1\n",
    "\n",
    "    if order == 2:\n",
    "\n",
    "        for key, value in pi_1.items():\n",
    "\n",
    "            #normalize by the order 2 count\n",
    "            pi_1[key] = float(float(value)/float(order2count))\n",
    "\n",
    "        return pi_1\n",
    "\n",
    "    else:\n",
    "        # iterate through the keys and values\n",
    "        for key, value in pi_2.items():\n",
    "\n",
    "            # value is a dict, whose \"values\" are counts\n",
    "            for tag, count in value.items():\n",
    "\n",
    "                #normalize by the order 3 count\n",
    "                pi_2[key][tag] = float(float(count)/float(order3count))\n",
    "\n",
    "        return pi_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function compute_initial_distribution.<locals>.<lambda> at 0x7fd19bed9700>, {'DT': defaultdict(<class 'int'>, {'JJ': 0.5}), 'NNP': defaultdict(<class 'int'>, {'NNPS': 0.5})})\n",
      "defaultdict(<class 'int'>, {'DT': 0.5, 'NNP': 0.5})\n"
     ]
    }
   ],
   "source": [
    "######################## TESTING COMPUTE INTIAL DISTRIBUTION ##############################\n",
    "littledata = read_pos_file('littledata.txt')\n",
    "print(compute_initial_distribution(littledata[0], 3))\n",
    "#expect DT-JJ to be 1/2 and NNP-NNPS to be 1/2\n",
    "#passes\n",
    "\n",
    "littledata = read_pos_file('littledata.txt')\n",
    "print(compute_initial_distribution(littledata[0], 2))\n",
    "# #expect DT to be 1/2 and NNP to be 1/2\n",
    "# #passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_emission_probabilities(unique_words, unique_tags, W, C):\n",
    "    \"\"\"\n",
    "    Function computes the emmission matrix for different parts of speech given training data\n",
    "\n",
    "    Input: unique_words: a set of unique words returned by the read_pos function\n",
    "           unique_tags : a set of unique tags returned by the read_pos function\n",
    "           W : the C(ti, wi) dictionary returned by the function compute_counts\n",
    "           C : the C(ti) dictionary returned by the function compute_counts\n",
    "\n",
    "    Output: emission matrix, a 2d dict where the keys are parts of speech\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #initialize the emission matrix\n",
    "    emission_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "\n",
    "    # tags are keys in the emission matrix\n",
    "    for tag in unique_tags:\n",
    "        for word in W[tag].keys():\n",
    "\n",
    "                #caluculate emission prob\n",
    "                emission_matrix[tag][word] = float(float(W[tag][word])/float(C[tag]))\n",
    "\n",
    "    return emission_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function compute_emission_probabilities.<locals>.<lambda> at 0x7fd1a441b160>, {'VBN': defaultdict(<class 'int'>, {'enacted': 1.0}), 'CD': defaultdict(<class 'int'>, {'1933': 0.5, '1936': 0.5}), 'VBD': defaultdict(<class 'int'>, {'was': 0.5, 'came': 0.5}), 'NN': defaultdict(<class 'int'>, {'series': 1.0}), 'JJ': defaultdict(<class 'int'>, {'domestic': 0.5, 'few': 0.5}), ',': defaultdict(<class 'int'>, {',': 1.0}), 'NNS': defaultdict(<class 'int'>, {'programs': 1.0}), 'NNP': defaultdict(<class 'int'>, {'New': 0.3333333333333333, 'Deal': 0.3333333333333333, 'United': 0.3333333333333333}), '.': defaultdict(<class 'int'>, {'.': 1.0}), 'CC': defaultdict(<class 'int'>, {'and': 1.0}), 'IN': defaultdict(<class 'int'>, {'of': 0.3333333333333333, 'in': 0.3333333333333333, 'between': 0.3333333333333333}), 'RB': defaultdict(<class 'int'>, {'later': 1.0}), 'DT': defaultdict(<class 'int'>, {'The': 0.25, 'a': 0.5, 'the': 0.25}), 'WDT': defaultdict(<class 'int'>, {'that': 1.0}), 'NNPS': defaultdict(<class 'int'>, {'States': 1.0})})\n"
     ]
    }
   ],
   "source": [
    "##################################### TESTING COMPUTE EMMISSION PROBABILITIES ######################\n",
    "tinydata = read_pos_file('onesentence.txt')\n",
    "unique_words2 = tinydata[1]\n",
    "unique_tags2 = tinydata[2]\n",
    "w1 = compute_counts(tinydata[0], 2)[1]\n",
    "C1 = compute_counts(tinydata[0], 2)[2]\n",
    "print(compute_emission_probabilities(unique_words2, unique_tags2, w1, C1))\n",
    "# expect a dict with each word corresponding to its emmision prob\n",
    "# passes\n",
    "\n",
    "onethousand = read_pos_file(\"onethousandlines.txt\")\n",
    "unique_tags = onethousand[2]\n",
    "unique_words = onethousand[1]\n",
    "c1 = compute_counts(onethousand[0], 3)[1]\n",
    "c2 = compute_counts(onethousand[0], 3)[2]\n",
    "# print(compute_emission_probabilities(unique_words, unique_tags, c1, c2))\n",
    "# # expect a dict with each word corresponding to its emmision prob\n",
    "# # passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lambdas(unique_tags, num_tokens, C1, C2, C3, order):\n",
    "    \"\"\"\n",
    "    Function implements the Algorithm Compute_Lambdas \n",
    "\n",
    "    Inputs: unique_tags: a set of unique tags returned by the read_pos function\n",
    "            numtokens : number of words in the training corpus\n",
    "            C1 : C(ti), The number of times tag ti appears\n",
    "            C2: C(ti-1, ti), the Number of times the sequence ti -1, ti appears \n",
    "            C3: C(ti-2, ti-1, ti) the number of times the sequence ti-2, ti-1, ti appears\n",
    "\n",
    "    Outputs: A list that contains lambda1, lambda2, lambda2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    lambdas = [0.0,0.0,0.0]\n",
    "    counter = 0\n",
    "    # only if order is 3 do we consider trigrams\n",
    "    if order == 3:\n",
    "\n",
    "        # access ti -2\n",
    "        for timinus2, value in C3.items():\n",
    "\n",
    "            # access ti-1\n",
    "            for timinus1, value2 in value.items():\n",
    "\n",
    "                # access ti\n",
    "                for ti, count in value2.items():\n",
    "\n",
    "                    counter += 1\n",
    "\n",
    "                    # initialize the argmax\n",
    "                    argmax = 0\n",
    "                    max_alpha = 0\n",
    "\n",
    "                    # calculate the alpha scores\n",
    "                    for i in range(3):\n",
    "                        if i == 0:\n",
    "                            if float(num_tokens) == 0:\n",
    "                                alpha = 0\n",
    "                            else:\n",
    "                                alpha = float(float(C1[ti] - 1)/float(num_tokens))\n",
    "\n",
    "                        if i == 1:\n",
    "                            if float(C1[timinus1] - 1) == 0:\n",
    "                                alpha = 0\n",
    "                            else:\n",
    "                                #print \"hey\", C2[timinus1][ti]\n",
    "                                #print C1[timinus1]\n",
    "                                alpha = float(float(C2[timinus1][ti] - 1)/float(C1[timinus1] - 1))\n",
    "\n",
    "                        if i == 2: \n",
    "                            if float(C2[timinus2][timinus1] - 1) == 0:\n",
    "                                alpha = 0\n",
    "                            else:\n",
    "                                alpha = float(float(C3[timinus2][timinus1][ti] - 1)/float(C2[timinus2][timinus1] - 1))\n",
    "\n",
    "                        # find the biggest alpha\n",
    "                        if alpha > max_alpha:\n",
    "                            max_alpha = alpha\n",
    "                            argmax = i\n",
    "\n",
    "                    # increment alpha\n",
    "                    lambdas[argmax] += C3[timinus2][timinus1][ti]\n",
    "\n",
    "        # calculate the lambda values\n",
    "\n",
    "        lambdas_sum = sum(lambdas)\n",
    "        for i in range(order):\n",
    "            lambdas[i] = float(float(lambdas[i])/float(lambdas_sum))\n",
    "\n",
    "        return lambdas\n",
    "\n",
    "    # if order is equal to 2\n",
    "    else:\n",
    "\n",
    "        #access ti- 1 and ti\n",
    "        for timinus1, value2 in C2.items():\n",
    "            for ti, count in value2.items():\n",
    "\n",
    "                argmax = 0\n",
    "                max_alpha = 0\n",
    "\n",
    "                #calculate alpha scores\n",
    "                for i in range(2):\n",
    "                    if i == 0:\n",
    "                        if float(num_tokens) == 0:\n",
    "                            alpha = 0\n",
    "                        else:\n",
    "                            alpha = float(float(C1[ti] - 1)/float(num_tokens))\n",
    "                    if i == 1:\n",
    "                        if float(C1[timinus1] - 1) == 0:\n",
    "                            alpha = 0\n",
    "                        else:\n",
    "                            alpha = float(float(C2[timinus1][ti] - 1)/float(C1[timinus1] - 1))\n",
    "\n",
    "                    if alpha > max_alpha:\n",
    "                        max_alpha = alpha\n",
    "                        argmax = i\n",
    "\n",
    "                # increment lambda\n",
    "                lambdas[argmax] += C2[timinus1][ti]\n",
    "\n",
    "        # calculate the final lambda values\n",
    "        lambdas_sum = sum(lambdas)\n",
    "\n",
    "        for i in range(order):\n",
    "            lambdas[i] = float(float(lambdas[i])/float(lambdas_sum))\n",
    "\n",
    "        return lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20472736536501893, 0.7952726346349811, 0.0]\n",
      "[0.9583333333333334, 0.041666666666666664, 0.0]\n"
     ]
    }
   ],
   "source": [
    "####################################### TESTING COMPUTE LAMBDAS #######################################################\n",
    "alldata = read_pos_file(\"training.txt\")\n",
    "counts = compute_counts(alldata[0], 3)\n",
    "unique_tags = alldata[2]\n",
    "unique_words = alldata[1]\n",
    "print(compute_lambdas(unique_tags, counts[0], counts[2], counts[3], counts[4], 2))\n",
    "# expect 2 lambda values that are not 0 that sum to 1\n",
    "# passes\n",
    "\n",
    "tinydata = read_pos_file('onesentence.txt')\n",
    "unique_tags = tinydata[2]\n",
    "counts = compute_counts(tinydata[0], 3)\n",
    "print(compute_lambdas(unique_tags, counts[0], counts[2], counts[3], counts[4], 3))\n",
    "# expect 2 lambda values that are not 0 that sum to 1\n",
    "# passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_matrix(training_data, unique_tags, order, use_smoothing):\n",
    "    \"\"\"\n",
    "    Input: training_data, a file containing training data, \n",
    "        unique tags, a set of unique tags in the data, \n",
    "        order, the order of the HMM\n",
    "        use smoothing: a boolean paramater\n",
    "\n",
    "    Output: transistion matrix: a matrix that contains the transistion probabiility between states\n",
    "    \"\"\"\n",
    "    # order\n",
    "    if order == 2:\n",
    "        # get the counts\n",
    "        # counts = compute_counts(training_data, 2)\n",
    "        counts_trigram = compute_counts(training_data, 3)\n",
    "\n",
    "        # get the number of tokens\n",
    "        num_tokens = counts_trigram[0]\n",
    "\n",
    "        # if smoothing is true, compute the appropriate lambda values\n",
    "        if use_smoothing == True:\n",
    "            lambdas = compute_lambdas(unique_tags, num_tokens, counts_trigram[2], counts_trigram[3], counts_trigram[4], order)\n",
    "        else:\n",
    "            lambdas = [0, 1, 0]\n",
    "\n",
    "        # compute the transition matrix\n",
    "        transition_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "        # obtain ti - 1\n",
    "        for timinus1 in unique_tags:\n",
    "\n",
    "            # obtain ti\n",
    "            for ti in unique_tags:\n",
    "\n",
    "                term1 = (float(lambdas[1])*float(counts_trigram[3][timinus1][ti])/float(counts_trigram[2][timinus1]))\n",
    "                term2 = (float(lambdas[0])*float(counts_trigram[2][ti])/float(num_tokens))\n",
    "\n",
    "                transition_matrix[timinus1][ti] = float(term1) + float(term2)\n",
    "\n",
    "        return transition_matrix\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        #get your counts\n",
    "        counts_trigram = compute_counts(training_data, 3)\n",
    "\n",
    "        # get the number of tokens\n",
    "        num_tokens = counts_trigram[0]\n",
    "\n",
    "        # if smoothing is true, compute the appropriate lambda values\n",
    "        if use_smoothing == True:\n",
    "            lambdas = compute_lambdas(unique_tags, num_tokens, counts_trigram[2], counts_trigram[3], counts_trigram[4], order)\n",
    "        else:\n",
    "            lambdas = [0.0, 0.0, 1.0]\n",
    "\n",
    "        # compute the transition matrix \t\n",
    "        transition_matrix = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "        for timinus2 in unique_tags:\n",
    "\n",
    "            # obtain ti - 1\n",
    "            for timinus1 in unique_tags:\n",
    "\n",
    "                # obtain ti\n",
    "                for ti in unique_tags:\n",
    "\n",
    "                    # calculate term 1 of the equation\n",
    "                    if float(counts_trigram[3][timinus2][timinus1]) == 0:\n",
    "                        term1 = 0\n",
    "                    else:\n",
    "                        term1 = (float(lambdas[2])*float(counts_trigram[4][timinus2][timinus1][ti]))/float(counts_trigram[3][timinus2][timinus1])\n",
    "\n",
    "                    #caluculate term 2 of the equations\n",
    "                    if float(counts_trigram[2][timinus1]) == 0:\n",
    "                        term2 =0\n",
    "                    else:\n",
    "                        term2 = (float(lambdas[1])*float(counts_trigram[3][timinus1][ti])/float(counts_trigram[2][timinus1]))\n",
    "\n",
    "                    # calculate term 3 of the equation\n",
    "                    if float(num_tokens) == 0:\n",
    "                        term3 =0\n",
    "                    else:\n",
    "                        term3 = (float(lambdas[0])*float(counts_trigram[2][ti])/float(num_tokens))\n",
    "\n",
    "                    #calculate the full probability\n",
    "                    transition_matrix[timinus2][timinus1][ti] = term1 + term2 + term3\n",
    "\n",
    "        return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function compute_transition_matrix.<locals>.<lambda> at 0x7fd1803628b0>, {'VBN': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 0.0, 'IN': 1.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.0}), 'CD': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.5, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 0.5, 'IN': 0.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.0}), 'VBD': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 0.0, 'IN': 0.0, 'RB': 0.5, 'DT': 0.5, 'WDT': 0.0, 'NNPS': 0.0}), 'NN': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 0.0, 'IN': 1.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.0}), 'JJ': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.5, 'NNP': 0.0, '.': 0.0, 'CC': 0.0, 'IN': 0.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.5, 'NNPS': 0.0}), ',': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 1.0, 'IN': 0.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.0}), 'NNS': defaultdict(<class 'int'>, {'VBN': 1.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 0.0, 'IN': 0.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.0}), 'NNP': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.3333333333333333, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.3333333333333333, '.': 0.0, 'CC': 0.0, 'IN': 0.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.3333333333333333}), '.': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 0.0, 'IN': 0.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.0}), 'CC': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.5, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 0.0, 'IN': 0.0, 'RB': 0.0, 'DT': 0.5, 'WDT': 0.0, 'NNPS': 0.0}), 'IN': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.3333333333333333, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.3333333333333333, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 0.0, 'IN': 0.0, 'RB': 0.0, 'DT': 0.3333333333333333, 'WDT': 0.0, 'NNPS': 0.0}), 'RB': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 1.0, 'CC': 0.0, 'IN': 0.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.0}), 'DT': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.25, 'JJ': 0.25, ',': 0.0, 'NNS': 0.0, 'NNP': 0.5, '.': 0.0, 'CC': 0.0, 'IN': 0.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.0}), 'WDT': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 1.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 0.0, 'IN': 0.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.0}), 'NNPS': defaultdict(<class 'int'>, {'VBN': 0.0, 'CD': 0.0, 'VBD': 0.0, 'NN': 0.0, 'JJ': 0.0, ',': 0.0, 'NNS': 0.0, 'NNP': 0.0, '.': 0.0, 'CC': 0.0, 'IN': 1.0, 'RB': 0.0, 'DT': 0.0, 'WDT': 0.0, 'NNPS': 0.0})})\n"
     ]
    }
   ],
   "source": [
    "################################################# TESTING COMPUTE TRANSISTION MATRIX ##########################################################\n",
    "\n",
    "tinydata = read_pos_file('onesentence.txt')\n",
    "unique_tags = tinydata[2]\n",
    "print(compute_transition_matrix(tinydata[0], unique_tags, 2, False))\n",
    "# EXPECTS a 2d dict of transistion probs from one state to another\n",
    "# passes\n",
    "\n",
    "tinydata = read_pos_file('onesentence.txt')\n",
    "unique_tags = tinydata[2]\n",
    "#print(compute_transition_matrix(tinydata[0], unique_tags, 3, True))\n",
    "# EXPECTS a 3d dict of transistion probs from two state to another\n",
    "# passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hmm(training_data, unique_tags, unique_words, order, use_smoothing):\n",
    "    \"\"\"\n",
    "    Creates a fully trained Hidden Markov Model\n",
    "\n",
    "    Inputs: training_data : a full training corpus,\n",
    "            unique_tags : a set of parts of speech found in the training corpus\n",
    "            unique_words : a set of words found in the training corpus\n",
    "            order : the order of the markov chain\n",
    "            Use_smoothing : a boolean parameter \n",
    "    Outputs: a fully trained HMM object\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # build an order 2 markov model\n",
    "\n",
    "    counts = compute_counts(training_data, order)\n",
    "\n",
    "    #compute the initial distribution\n",
    "    initial_distribution = compute_initial_distribution(training_data, order)\n",
    "\n",
    "    #compute the emission matrix\n",
    "    W_dict = counts[1]\n",
    "    C_dict = counts[2]\n",
    "    emission_matrix = compute_emission_probabilities(unique_words, unique_tags, W_dict, C_dict)\n",
    "\n",
    "    #build a transition matrix\n",
    "    transition_matrix = compute_transition_matrix(training_data, unique_tags, order, use_smoothing)\n",
    "\n",
    "    #build the hmm\n",
    "    my_hmm = HMM(order, initial_distribution, emission_matrix, transition_matrix)\n",
    "\n",
    "    return my_hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## Testing Build HMM ################################################################\n",
    "\n",
    "onethousand = read_pos_file(\"onethousandlines.txt\")\n",
    "unique_tags = onethousand[2]\n",
    "unique_words = onethousand[1]\n",
    "#print(build_hmm(onethousand[0], unique_tags, unique_words, 2, True))\n",
    "#expect am HMM object\n",
    "#passes\n",
    "\n",
    "tinydata = read_pos_file('onesentence.txt')\n",
    "unique_tags = tinydata[2]\n",
    "unique_words = tinydata[1]\n",
    "model = build_hmm(tinydata[0], unique_tags, unique_words, 3, False)\n",
    "#Expect an HMM Object \n",
    "#View the HMM Attributes\n",
    "#print( \"Order\", model.order)\n",
    "#print( \"Intial Dist\", model.initial_distribution)\n",
    "#print( \"Emmission Matrix\", model.emission_matrix)\n",
    "#print( \"trans mat\", model.transition_matrix)\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hmm(hmm, sentence):\n",
    "    \"\"\"\n",
    "    Function updates HMM based on new words it encounters\n",
    "    Input: an hmm object and a sentence, a list of strings ending with a period\n",
    "    Output: An updated hmm object\n",
    "    \"\"\"\n",
    "    #get the attributes of the hidden markov model\n",
    "    order = hmm.order\n",
    "    initial_distribution = hmm.initial_distribution\n",
    "    emission_matrix = hmm.emission_matrix\n",
    "    transition_matrix = hmm.transition_matrix\n",
    "\n",
    "\n",
    "    # get all the words\n",
    "    unique_words = []\n",
    "\n",
    "    for pos in emission_matrix:\n",
    "\n",
    "        for word in emission_matrix[pos]:\n",
    "\n",
    "\n",
    "            # add each word that the model has seen into unique words\n",
    "\n",
    "            unique_words.append(word)\n",
    "\n",
    "    #bool flag for if any new words were encountered\n",
    "    new_word = False\n",
    "\n",
    "    for word in sentence:\n",
    "\n",
    "        # if the word is new, we want to add update the HMM\n",
    "        if word not in unique_words:\n",
    "\n",
    "            # if we get to this line, we have a new word\n",
    "            new_word = True\n",
    "\n",
    "            for part_of_speech in emission_matrix:\n",
    "\n",
    "                for seen_word in emission_matrix[part_of_speech]:\n",
    "\n",
    "                    # increment the each word by the same amount\n",
    "\n",
    "                    emission_matrix[part_of_speech][seen_word] +=  0.00001\n",
    "\n",
    "                # add the new word to each part of speech with a small probability\n",
    "\n",
    "                emission_matrix[part_of_speech][word] =  0.00001\n",
    "\n",
    "\n",
    "    if new_word == True:\n",
    "\n",
    "        #begin the normalization process\n",
    "        for tag in emission_matrix:\n",
    "\n",
    "            # find the normalizing term\n",
    "            normalizer = sum(emission_matrix[tag].values())\n",
    "\n",
    "            # normalize each word\n",
    "\n",
    "            for finalword in emission_matrix[tag]:\n",
    "\n",
    "                emission_matrix[tag][finalword] = float(emission_matrix[tag][finalword])/float(normalizer)\n",
    "\n",
    "        # updated_model = HMM(order, initial_distribution, emission_matrix, transition_matrix)\n",
    "\n",
    "    return HMM(order, initial_distribution, emission_matrix, transition_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "############################################################# Testing Update HMM \n",
    "alldata = read_pos_file(\"training.txt\")\n",
    "unique_tags = alldata[2]\n",
    "unique_words = alldata[1]\n",
    "hiddenmarkovmodel = build_hmm(alldata[0], unique_tags, unique_words, 2, False)\n",
    "print (hiddenmarkovmodel.order)\n",
    "#expect a new hmm of order 2\n",
    "#passes\n",
    "\n",
    "alldata = read_pos_file(\"training.txt\")\n",
    "unique_tags = alldata[2]\n",
    "unique_words = alldata[1]\n",
    "hiddenmarkovmodel = build_hmm(alldata[0], unique_tags, unique_words, 3, False)\n",
    "print (hiddenmarkovmodel.order)\n",
    "#expect a new hmm of order 3\n",
    "#passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-inf\n",
      "4.23410650459726\n"
     ]
    }
   ],
   "source": [
    "def log(number):\n",
    "    \"\"\"\n",
    "    Caluculates the Logarithm of a number, and returns -inf in the number is 0\n",
    "    Inputs : number, a real number\n",
    "    Output : The log of a number, whihc is a real number of -inf\n",
    "    \"\"\"\n",
    "\n",
    "    # log of 0 is negative infinity\n",
    "\n",
    "    if number == 0:\n",
    "        return float(\"-inf\")\n",
    "    else:\n",
    "        return float(math.log(number))\n",
    "\n",
    "\n",
    "################################# Testing LOG  \n",
    "print( log(0))\n",
    "# Expect - inf\n",
    "# passes\n",
    "\n",
    "print( log(69))\n",
    "# expect 4.2\n",
    "#passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_viterbi(hmm, sentence):\n",
    "    \"\"\"\n",
    "    Implements the Viterbi algorithm for the bigram model on an input HMM and a sentence (a list of words and the period at the end).\n",
    "\n",
    "    Input : HMM, an HMM object, and sentence, a list of words with a period at the end\n",
    "    Output: Tagged Words : a list of words and their tags\n",
    "    \"\"\"\n",
    "    #get the data from the HMM object\n",
    "    initial_distribution = hmm.initial_distribution\n",
    "    emission_matrix = hmm.emission_matrix\n",
    "    transition_matrix = hmm.transition_matrix\n",
    "\n",
    "\n",
    "    # init V and BP\n",
    "    V = defaultdict(lambda: defaultdict(int))\n",
    "    bp = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # length of the input sentence.\n",
    "    L = len(sentence)\n",
    "\n",
    "    # init the first column of the V matrix\n",
    "    for pos in emission_matrix:\n",
    "\n",
    "        #caluclate the first column of the matrix \n",
    "        pi_sub_l = log(initial_distribution[pos])\n",
    "        emissison_prob_x0 = log(emission_matrix[pos][sentence[0]])\n",
    "        V[pos][0] = pi_sub_l + emissison_prob_x0\n",
    "\n",
    "    for i in range(1, L):\n",
    "\n",
    "        # l is a part of speech, more generally, a markov state\n",
    "        for l in emission_matrix:\n",
    "\n",
    "            max_prob = -float(\"inf\")\n",
    "            argmax = None\n",
    "\n",
    "            # find the different l_prime values that can be take, then determine the argmax and the max\n",
    "            for l_prime in emission_matrix:\n",
    "\n",
    "                transition_probability = log(transition_matrix[l_prime][l])\n",
    "\n",
    "                previous_prob = V[l_prime][i-1]\n",
    "                possible_max = transition_probability + previous_prob\n",
    "\n",
    "                #update l prime\n",
    "                if possible_max > max_prob:\n",
    "                    max_prob = possible_max\n",
    "                    argmax = l_prime\n",
    "\n",
    "            # if none\n",
    "            if argmax == None:\n",
    "                for l_prime in emission_matrix.keys():\n",
    "                    if V[l_prime][i-1]>= max_prob:\n",
    "                        max_prob = V[l_prime][i-1]\n",
    "                        argmax = l_prime\n",
    "\n",
    "            emission_prob = log(emission_matrix[l][sentence[i]])\n",
    "\n",
    "            #update V and BP\n",
    "            V[l][i] = emission_prob + max_prob\n",
    "            bp[l][i] = argmax\n",
    "\n",
    "\n",
    "    # begin traceback\n",
    "    argmax2 = None\n",
    "    max_val_holder = -float(\"inf\")\n",
    "\n",
    "    #access the first element in the Sequence\n",
    "    for l_prime_pos in emission_matrix:\n",
    "        v_mat_entry = V[l_prime_pos][L-1]\n",
    "\n",
    "        #get highest entry\n",
    "        if v_mat_entry > max_val_holder:\n",
    "            max_val_holder = v_mat_entry\n",
    "            argmax2 = l_prime_pos\n",
    "\n",
    "    sentence[L-1] = (sentence[L-1], argmax2)\n",
    "\n",
    "    #traceback\n",
    "    for i in range(L-2, -1, -1):\n",
    "        zi = bp[sentence[i+1][1]][i+1]\n",
    "        sentence[i] = (sentence[i], zi)\n",
    "\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('hips', 'NNS'), ('do', 'VBP'), ('not', 'RB'), ('lie', 'VB'), ('.', '.')]\n",
      "[('I', 'PRP'), ('hope', 'VBP'), ('I', 'PRP'), ('pass', 'VBP'), ('this', 'DT'), ('class', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "################################################################ TESTING BIGRAM VITERBI \n",
    "\n",
    "alldata = read_pos_file(\"training.txt\")\n",
    "unique_tags = alldata[2]\n",
    "unique_words = alldata[1]\n",
    "hiddenmarkovmodel = build_hmm(alldata[0], unique_tags, unique_words, 2, False)\n",
    "print (bigram_viterbi(hiddenmarkovmodel, [\"My\", \"hips\", \"do\", \"not\", \"lie\", \".\"]))\n",
    "# Expect Possesive, noun, verb, negator, verb, period\n",
    "# Passes\n",
    "\n",
    "alldata = read_pos_file(\"training.txt\")\n",
    "unique_tags = alldata[2]\n",
    "unique_words = alldata[1]\n",
    "hiddenmarkovmodel = build_hmm(alldata[0], unique_tags, unique_words, 2, False)\n",
    "print (bigram_viterbi(hiddenmarkovmodel, [\"I\", \"hope\", \"I\", \"pass\", \"this\", \"class\", \".\"]))\n",
    "# Expect Pronoun, verb, Pronoun, verb, determiner, noun, perios\n",
    "# Passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def trigram_viterbi(hmm, sentence):\n",
    "    \"\"\"\n",
    "    Implements the Viterbi algorithm for the treigram model on an input HMM and a sentence (a list of words and the period at the end).\n",
    "\n",
    "    Input : HMM, an HMM object, and sentence, a list of words with a period at the end\n",
    "    Output: Tagged Words : a list of words and their tags\n",
    "    \"\"\"\n",
    "    #get the data from the HMM object\n",
    "    initial_distribution = hmm.initial_distribution\n",
    "    emission_matrix = hmm.emission_matrix\n",
    "    transition_matrix = hmm.transition_matrix\n",
    "\n",
    "    # init V and BP\n",
    "    V = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "    bp = defaultdict(lambda: defaultdict(lambda: defaultdict(int)))\n",
    "\n",
    "    # length of the input sentence.\n",
    "    L = len(sentence)\n",
    "\n",
    "    #being building the V matrix\n",
    "\n",
    "    # init the l prime of the V matrix\n",
    "    for pos_l_prime in emission_matrix:\n",
    "\n",
    "        #init the l state\n",
    "        for pos_l in emission_matrix:\n",
    "\n",
    "            #caucluate the first column/plane in the 3d matrix\n",
    "\n",
    "            pi_sub_lprime_l = log(initial_distribution[pos_l_prime][pos_l])\n",
    "            emissison_prob_x0 = log(emission_matrix[pos_l_prime][sentence[0]])\n",
    "            emissison_prob_x1 = log(emission_matrix[pos_l][sentence[1]])\n",
    "\n",
    "            # put the entry in the matrix\n",
    "            V[pos_l_prime][pos_l][1] = pi_sub_lprime_l + emissison_prob_x0 + emissison_prob_x1\n",
    "\n",
    "    for i in range(2, L):\n",
    "\n",
    "        # l is a part of speech, more generally, a markov state\n",
    "        for l_prime in emission_matrix:\n",
    "\n",
    "            # find the different l_prime values that can be take, then determine the argmax and the max\n",
    "            for l in emission_matrix:\n",
    "\n",
    "                bestmax = -float(\"inf\")\n",
    "                argmax = None\n",
    "\n",
    "                for l_double_prime in emission_matrix:\n",
    "\n",
    "                    #calculate possible entries\n",
    "\n",
    "                    transition_probability = log(transition_matrix[l_double_prime][l_prime][l])\n",
    "                    previous_prob = V[l_double_prime][l_prime][i-1]\n",
    "                    possible_max = transition_probability + previous_prob\n",
    "\n",
    "                    # update l double prime\n",
    "                    if possible_max > bestmax:\n",
    "                        bestmax = possible_max\n",
    "                        argmax = l_double_prime\n",
    "\n",
    "                #if none in matrix\n",
    "                if argmax == None:\n",
    "                    for l_double_prime in emission_matrix.keys():\n",
    "                        if V[l_double_prime][l_prime][i-1] >= bestmax:\n",
    "                            bestmax = V[l_double_prime][l_prime][i-1]\n",
    "                            argmax = l_double_prime\n",
    "\n",
    "                emission_prob = log(emission_matrix[l][sentence[i]])\n",
    "\n",
    "                #update V and BP\n",
    "\n",
    "                V[l_prime][l][i] = emission_prob + bestmax\n",
    "                bp[l_prime][l][i] = argmax\n",
    "\n",
    "\n",
    "    # begin traceback\n",
    "    ZL_minus_1 = None\n",
    "    ZL_minus_2 = None\n",
    "    max_val_holder= -float(\"inf\")\n",
    "\n",
    "    #access the first element in the Sequence\n",
    "    for state_1 in emission_matrix:\n",
    "        for state_2 in emission_matrix:\n",
    "            v_mat_entry = V[state_1][state_2][L-1]\n",
    "\n",
    "            #get highest entry\n",
    "            if v_mat_entry > max_val_holder:\n",
    "                max_val_holder = v_mat_entry\n",
    "                ZL_minus_1 = state_2\n",
    "                ZL_minus_2 = state_1\n",
    "\n",
    "    #init last Z values\n",
    "    sentence[L-1] = (sentence[L-1], ZL_minus_1)\n",
    "    sentence[L-2] = (sentence[L-2], ZL_minus_2)\n",
    "\n",
    "    #traceback\n",
    "    for i in range(L-3, -1, -1):\n",
    "        zi = bp[sentence[i+1][1]][sentence[i+2][1]][i+2]\n",
    "        sentence[i] = (sentence[i], zi)\n",
    "\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('the', 'DT'), ('machine', 'NN'), ('.', '.')]\n",
      "[('I', 'PRP'), ('am', 'VBP'), ('happy', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "######################################## TESTING TRIGRAM \n",
    "\n",
    "alldata = read_pos_file(\"training.txt\")\n",
    "unique_tags = alldata[2]\n",
    "unique_words = alldata[1]\n",
    "hiddenmarkovmodel = build_hmm(alldata[0], unique_tags, unique_words, 3, False)\n",
    "print (trigram_viterbi(hiddenmarkovmodel, [\"I\", \"am\", \"the\", \"machine\", \".\"]))\n",
    "# expecg Pronoun, verb, det, noun, period\n",
    "#passes\n",
    "\n",
    "alldata = read_pos_file(\"training.txt\")\n",
    "unique_tags = alldata[2]\n",
    "unique_words = alldata[1]\n",
    "hiddenmarkovmodel = build_hmm(alldata[0], unique_tags, unique_words, 3, False)\n",
    "print (trigram_viterbi(hiddenmarkovmodel, [\"I\", \"am\", \"happy\", \".\"]))\n",
    "# expecg Pronoun, verb, adj, period\n",
    "#passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANALYSIS CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(tagged_data, results):\n",
    "    '''\n",
    "    Computes similarity between the actual tagged data and the algorithm's tags\n",
    "    Input: tagged data, a list of tuples, and results, a list of tuples\n",
    "    '''\n",
    "\n",
    "    #init a numerator and a denominator\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for i in range(len(tagged_data)):\n",
    "\n",
    "            #add a denominator regardless\n",
    "            denominator += 1\n",
    "            if tagged_data[i] == results[i]:\n",
    "\n",
    "                #if we get a match, increment the numerator\n",
    "                numerator += 1\n",
    "\n",
    "    #calculate the percent accuracy\n",
    "    percent_accurate = float(numerator)/float(denominator)\n",
    "\n",
    "    return percent_accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "############################################ TESTING COMPUTE ACCURACY ############################\n",
    "a = [\"a\", \"a\", \"a\", \"a\"]\n",
    "b = [\"a\", \"b\", \"a\", \"a\"]\n",
    "print (compute_accuracy(a,b))\n",
    "# expect .75\n",
    "#passes\n",
    "\n",
    "a = [\"a\", \"a\", \"a\", \"a\"]\n",
    "b = [\"a\", \"a\", \"a\", \"a\"]\n",
    "print (compute_accuracy(a,b))\n",
    "# expect 1.0\n",
    "#passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_validate(training_data, percent, testdata_untagged, testdata_tagged, order, use_smoothing):\n",
    "    \"\"\"\n",
    "    Computes the out of sample accuracy of the POS tagging algorithm for bigram HMM\n",
    "    Inputs: a file training data, a percentage of data, untagged test data, tagged test data, the order of the markov chain, \n",
    "            a boolean parameter use smoothing\n",
    "\n",
    "    Output: Accuracy, a real number between 0 and 1\n",
    "    \"\"\"\n",
    "\n",
    "    #wrangle the data\n",
    "    my_data = wrangle_data(training_data, percent)\n",
    "    unique_words, unique_tags = get_unique_words_and_tags(my_data)\n",
    "\n",
    "    #build an HMM\n",
    "    old_hmm = build_hmm(my_data, unique_tags, unique_words, order, use_smoothing)\n",
    "    list_of_words, test_data_parsed = parse_test_file(testdata_untagged)\n",
    "\n",
    "    #update the HMM if need be\n",
    "    new_hmm = update_hmm(old_hmm, list_of_words)\n",
    "\n",
    "    # put the predidted tags into a master list\n",
    "    full_results = []\n",
    "    for sentence in test_data_parsed:\n",
    "        results = bigram_viterbi(new_hmm, sentence)\n",
    "        for tup in results:\n",
    "            full_results.append(tup)\n",
    "\n",
    "    # read in the validation data\n",
    "    validation_data = read_pos_file_modified(testdata_tagged)\n",
    "\n",
    "\n",
    "    #get the final accuracy\n",
    "    return compute_accuracy(validation_data, full_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7587268993839835\n",
      "0.6919917864476386\n"
     ]
    }
   ],
   "source": [
    "######################################## TESTING BIGRAM Validate ######################################\n",
    "print (bigram_validate(\"training.txt\", 0.01, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, True))\n",
    "print (bigram_validate(\"training.txt\", 0.01, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, False))\n",
    "\n",
    "#expect two decimals between 0-1 with the first value higher than the second\n",
    "# Passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigram_validate(training_data, percent, testdata_untagged, testdata_tagged, order, use_smoothing):\n",
    "    \"\"\"\n",
    "    Computes the out of sample accuracy of the POS tagging algorithm for bigram HMM\n",
    "    Inputs: a file training data, a percentage of data, untagged test data, tagged test data,\n",
    "            the order of the markov chain, a boolean parameter use smoothing\n",
    "\n",
    "    Output: Accuracy, a real number between 0 and 1\n",
    "    \"\"\"\n",
    "    #wrangle the data\n",
    "    my_data = wrangle_data(training_data, percent)\n",
    "    unique_words, unique_tags = get_unique_words_and_tags(my_data)\n",
    "\n",
    "    #build an hmm\n",
    "    old_hmm = build_hmm(my_data, unique_tags, unique_words, order, use_smoothing)\n",
    "    list_of_words, test_data_parsed = parse_test_file(testdata_untagged)\n",
    "\n",
    "    #update the HMM if any new words are encountered\n",
    "    new_hmm = update_hmm(old_hmm, list_of_words)\n",
    "\n",
    "    # put the algorithm's predictions into a master list\n",
    "    full_results = []\n",
    "    for sentence in test_data_parsed:\n",
    "        results = trigram_viterbi(new_hmm, sentence)\n",
    "        for tup in results:\n",
    "            full_results.append(tup)\n",
    "\n",
    "    #read in the validation data \n",
    "    validation_data = read_pos_file_modified(testdata_tagged)\n",
    "\n",
    "    #get an accuracy value\n",
    "    return compute_accuracy(validation_data, full_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7453798767967146\n",
      "0.6468172484599589\n"
     ]
    }
   ],
   "source": [
    "#################################### TESTING TRIGRAM VALIDATE \n",
    "print (trigram_validate(\"training.txt\", 0.01, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, True))\n",
    "print (trigram_validate(\"training.txt\", 0.01, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, False))\n",
    "\n",
    "# expect two decimals between 0-1 with the first value higher than the second\n",
    "# Passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6919917864476386, 0.8275154004106776, 0.8696098562628337, 0.9055441478439425, 0.9271047227926078, 0.9496919917864476, 0.9579055441478439]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[0.6919917864476386, 0.8275154004106776, 0.8696098562628337, 0.9055441478439425, 0.9271047227926078, 0.9496919917864476, 0.9579055441478439]\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################# EXPERIMENT ONE #####################################################\n",
    "\n",
    "'''\n",
    "In experiment one, we build seven bigram HMMs on the first 1%, 5%, 10%, 25%, 50%,\n",
    "75%, and 100% of the training corpus without smoothing and obtain 7 accuracy values\n",
    "'''\n",
    "onepercent_1 = bigram_validate(\"training.txt\", 0.01, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, False)\n",
    "fivepercent_1 = bigram_validate(\"training.txt\", 0.05, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, False)\n",
    "tenpercent_1 = bigram_validate(\"training.txt\", 0.1, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, False)\n",
    "twentyfivepercent_1 = bigram_validate(\"training.txt\", 0.25, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, False)\n",
    "fiftypercent_1 = bigram_validate(\"training.txt\", 0.5, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, False)\n",
    "seventyfivepercent_1 = bigram_validate(\"training.txt\", 0.75, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, False)\n",
    "onehundopercent_1 = bigram_validate(\"training.txt\", 1, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, False)\n",
    "experiment_1_results = [onepercent_1, fivepercent_1, tenpercent_1, \n",
    "                        twentyfivepercent_1, fiftypercent_1, seventyfivepercent_1, onehundopercent_1]\n",
    "print(experiment_1_results)\n",
    "\n",
    "\"\"\"\n",
    "[0.6919917864476386, 0.8275154004106776, 0.8696098562628337, 0.9055441478439425, 0.9271047227926078, 0.9496919917864476, 0.9579055441478439]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6468172484599589, 0.7905544147843943, 0.8357289527720739, 0.8593429158110883, 0.8952772073921971, 0.917864476386037, 0.9322381930184805]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[0.6468172484599589, 0.7905544147843943, 0.8357289527720739, 0.8593429158110883, 0.8952772073921971, 0.917864476386037, 0.9322381930184805]\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################ EXPERIMENT TWO #######################################################\n",
    "\n",
    "'''\n",
    "In experiment 2, we build 7 trigram HMMs on the first 1%, 5%, 10%, 25%, 50%,\n",
    "75%, and 100% of the training corpus without smoothing and obtain 7 accuracy values\n",
    "'''\n",
    "onepercent_2 = trigram_validate(\"training.txt\", 0.01, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, False)\n",
    "fivepercent_2 = trigram_validate(\"training.txt\", 0.05, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, False)\n",
    "tenpercent_2 = trigram_validate(\"training.txt\", 0.1, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, False)\n",
    "twentyfivepercent_2 = trigram_validate(\"training.txt\", 0.25, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, False)\n",
    "fiftypercent_2 = trigram_validate(\"training.txt\", 0.5, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, False)\n",
    "seventyfivepercent_2 = trigram_validate(\"training.txt\", 0.75, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, False)\n",
    "onehundopercent_2 = trigram_validate(\"training.txt\", 1, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, False)\n",
    "experiment_2_results = [onepercent_2, fivepercent_2, tenpercent_2, twentyfivepercent_2, fiftypercent_2, seventyfivepercent_2, onehundopercent_2]\n",
    "print(experiment_2_results)\n",
    "\n",
    "'''\n",
    "[0.6468172484599589, 0.7905544147843943, 0.8357289527720739, 0.8593429158110883, 0.8952772073921971, 0.917864476386037, 0.9322381930184805]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7587268993839835, 0.8603696098562629, 0.8901437371663244, 0.919917864476386, 0.9394250513347022, 0.9548254620123203, 0.9599589322381931]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[0.7587268993839835, 0.8603696098562629, 0.8901437371663244, 0.919917864476386, 0.9394250513347022, 0.9548254620123203, 0.9599589322381931]\\n\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################## EXPERIMENT 3 \n",
    "'''\n",
    "In experiment three, we build seven bigram HMMs on the first 1%, 5%, 10%, 25%, 50%,\n",
    "75%, and 100% of the training corpus with smoothing and obtain 7 accuracy values\n",
    "'''\n",
    "onepercent_3 = bigram_validate(\"training.txt\", 0.01, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, True)\n",
    "fivepercent_3 = bigram_validate(\"training.txt\", 0.05, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, True)\n",
    "tenpercent_3 = bigram_validate(\"training.txt\", 0.1, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, True)\n",
    "twentyfivepercent_3 = bigram_validate(\"training.txt\", 0.25, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, True)\n",
    "fiftypercent_3 = bigram_validate(\"training.txt\", 0.5, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, True)\n",
    "seventyfivepercent_3 = bigram_validate(\"training.txt\", 0.75, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, True)\n",
    "onehundopercent_3 = bigram_validate(\"training.txt\", 1, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 2, True)\n",
    "experiment_3_results = [onepercent_3, fivepercent_3, tenpercent_3, twentyfivepercent_3, fiftypercent_3, seventyfivepercent_3, onehundopercent_3]\n",
    "print (experiment_3_results)\n",
    "\n",
    "'''\n",
    "[0.7587268993839835, 0.8603696098562629, 0.8901437371663244, 0.919917864476386, 0.9394250513347022, 0.9548254620123203, 0.9599589322381931]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7453798767967146, 0.86652977412731, 0.8993839835728953, 0.9281314168377823, 0.9496919917864476, 0.9640657084188912, 0.9691991786447639]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[0.7453798767967146, 0.86652977412731, 0.8993839835728953, 0.9281314168377823, 0.9496919917864476, 0.9640657084188912, 0.9691991786447639]\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################################### EXPERIMENT 4 \n",
    "'''\n",
    "In experiment 2, we build 7 trigram HMMs on the first 1%, 5%, 10%, 25%, 50%,\n",
    "75%, and 100% of the training corpus with smoothing and obtain 7 accuracy values\n",
    "'''\n",
    "\n",
    "onepercent_4 = trigram_validate(\"training.txt\", 0.01, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, True)\n",
    "fivepercent_4 = trigram_validate(\"training.txt\", 0.05, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, True)\n",
    "tenpercent_4 = trigram_validate(\"training.txt\", 0.1, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, True)\n",
    "twentyfivepercent_4 = trigram_validate(\"training.txt\", 0.25, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, True)\n",
    "fiftypercent_4 = trigram_validate(\"training.txt\", 0.5, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, True)\n",
    "seventyfivepercent_4 = trigram_validate(\"training.txt\", 0.75, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, True)\n",
    "onehundopercent_4 = trigram_validate(\"training.txt\", 1, \"testdata_untagged.txt\", \"testdata_tagged.txt\", 3, True)\n",
    "experiment_4_results = [onepercent_4, fivepercent_4, tenpercent_4, twentyfivepercent_4, fiftypercent_4, seventyfivepercent_4, onehundopercent_4]\n",
    "print(experiment_4_results)\n",
    "\n",
    "'''\n",
    "[0.7453798767967146, 0.86652977412731, 0.8993839835728953, 0.9281314168377823, 0.9496919917864476, 0.9640657084188912, 0.9691991786447639]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing package\n",
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "# create data\n",
    "x = [1,5,10,25,50, 75, 100]\n",
    "y1 = experiment_1_results\n",
    "y2 = experiment_2_results\n",
    "y3 = experiment_3_results\n",
    "y4 = experiment_4_results\n",
    "# plot lines\n",
    "plt.plot(x, y1, label = \"Bigram No-Smoothing\")\n",
    "plt.plot(x, y2, label = \"Trigram No-Smoothing\")\n",
    "plt.plot(x, y3, label = \"Bigram Smoothing\")\n",
    "plt.plot(x, y4, label = \"Trigram Smoothing\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
